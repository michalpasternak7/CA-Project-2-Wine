{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation of Sprint 3. So the first line of code are a copied from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "red_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "white_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove lines that have all values duplicated\n",
    "red_wine.drop_duplicates(inplace=True)\n",
    "white_wine.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with all wines for later analysis\n",
    "\n",
    "# add color of wine as parameter\n",
    "red_wine['red'] = 1\n",
    "red_wine['white'] = 0\n",
    "white_wine['white'] = 1\n",
    "white_wine['red'] = 0\n",
    "\n",
    "# combine the wine dfs\n",
    "wine = pd.concat([red_wine, white_wine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the rename method to change all columns names lowercase and add an underscore if they are made of 2 words\n",
    "wine.rename(str.lower, axis='columns', inplace=True)  # make the names lowercase\n",
    "wine.columns = wine.columns.str.replace(' ', '_')       # replace space with underscore in column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "5       1\n",
      "       ..\n",
      "4893    0\n",
      "4894    0\n",
      "4895    0\n",
      "4896    0\n",
      "4897    0\n",
      "Name: red, Length: 5320, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = wine # --> the features we will keep to build our model\n",
    "y = X['red'] # --> what you're trying to predict\n",
    "X.drop(['red', 'white'],axis=1,inplace=True)\n",
    "print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epic 1: Pick your (ML) fighter\n",
    "After all your hard work, it is finally time to choose the ML algorithm that you'll use to answer your question. Not all algorithms are created equal, so selecting one depends on the problem at hand: Is it a supervised or unsupervised learning problem? Is it a regression, a classification or a forecasting problem?\n",
    "\n",
    "We won't be going into the details of each algorithm in this guide (they're too many!), but bear in mind that you will have to apply several algorithms to the same data set and compare the results.\n",
    "\n",
    "**Read the Machine Learning Fundamentals chapters if you haven't.**\n",
    "\n",
    "Please refer to the ML algorithm map we saw during the spike. You can [find it here](https://www.mdpi.com/2075-4426/11/1/32/htm) (figure 1).\n",
    "\n",
    "We suggest that you explore the different algorithms and their pros and cons. Here is a [wonderful article](https://builtin.com/data-science/tour-top-10-algorithms-machine-learning-newbies) that explains in simple terms different algorithms and their applications. Also, in [this link](https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html), you can find more tips about how to choose your algorithm.\n",
    "\n",
    "**Hint: start with a simple logistic regression**\n",
    "\n",
    "### Fit your model\n",
    "Fitting the model means training the model on training data using the .fit method provided in sklearn. For illustration purposes, we will use Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michalpasternak/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "# Fit the model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test set\n",
    "Now you will apply the .predict() method to make predictions on test data. These predictions are stored in 'pred_lr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no point in making predictions if you do not evaluate the results. You will now measure the effectiveness of your trained models to determine and compare how well a model performs. These model-evaluation techniques are crucial in machine learning model development.\n",
    "\n",
    "Here’s a list of evaluators used for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epic 2: Evaluate your model\n",
    "\n",
    "### The confusion matrix\n",
    "The confusion matrix is not an evaluation metric but allows you to have a tabular visualisation of the predictions made by your model vs their actual class.\n",
    "\n",
    "In this example, we want to classify if wine was red or white. After we’ve done the predictions with our model, we want to understand how many predictions were correct in each category and were wrong.\n",
    "\n",
    "One of the easiest ways to visualise your results is with a confusion matrix, as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[768   7]\n",
      " [ 10 279]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics are calculated by using true and false positives, true and false negatives:\n",
    "\n",
    "- **TN / True Negative**: when a case was negative and predicted negative\n",
    "- **TP / True Positive**: when a case was positive and predicted positive\n",
    "- **FN / False Negative**: when a case was positive but predicted negative\n",
    "- **FP / False Positive**: when a case was negative but predicted positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy score\n",
    "Probably the simplest valuation metric, defined as the number of correct predictions divided by the total number of predictions. But be very careful! When you have **imbalance data**, where there are more samples or one category than of another, **the accuracy score can be misleading**.\n",
    "\n",
    "To illustrate this, we’ll see a commonly used example: imagine you work at a hospital, and you have created an ML model to classify tumours between benign and malign. You run your model and these are your results:\n",
    "- Of the 91 benign tumours, the model correctly identifies 90 as benign. That's good.\n",
    "- However, of the 9 malignant tumours, the model only correctly identifies 1 as malignant, meaning that 8 out of 9 malignancies go undiagnosed!\n",
    "\n",
    "While 91% accuracy may seem good at first glance, your model has zero predictive ability to distinguish malignant tumours from benign tumours, making it useless.\n",
    "\n",
    "**Accuracy Score = (TP + TN) / (TP + TN + FP + FN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9840225563909775\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score:\", accuracy_score(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classification report\n",
    "A Classification report measures the quality of predictions from a classification algorithm, reflecting how many predictions are accurate, whether they're true or false. The report also shows the main classification metrics precision, recall and f1-score on a per-class basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         red       0.99      0.99      0.99       775\n",
      "       white       0.98      0.97      0.97       289\n",
      "\n",
      "    accuracy                           0.98      1064\n",
      "   macro avg       0.98      0.98      0.98      1064\n",
      "weighted avg       0.98      0.98      0.98      1064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_lr, target_names=[\"red\",\"white\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do these metrics mean? Here’s a brief description.\n",
    "\n",
    "*Precision*:\n",
    "\n",
    "It says the accuracy of correct positive predictions (True Positive) in comparison to the sum of True and False Positives. The best value is 1 and the worst value is 0.\n",
    "\n",
    "**Precision = TP / (TP + FP)**\n",
    "\n",
    "The False Positives are those predictions that were Negative but our model predicted them as Positives. So, from all the predictions that were classified as Positive, how many were in reality Negative?\n",
    "\n",
    "*Recall*:\n",
    "\n",
    "It says the fraction of True Positives that were correctly identified. The best value is 1 and the worst value is 0.\n",
    "\n",
    "**Recall = TP / (TP + FN)**\n",
    "\n",
    "The False Negatives are those predictions that were True but our model predicted them as Negatives. So, from all the predictions that should have been True, what fraction was identified as such?\n",
    "\n",
    "*F1 Score*:\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances) As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n",
    "\n",
    "**F1 Score = 2((precisionrecall)/(precision+recall))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen-Kappa Score\n",
    "Kappa is an important measure of classifier performance, **especially on an imbalanced data set and multi-class problems**. It expresses the level of agreement between two annotators on a classification problem, one annotator being your ML model, the second classifier being a model that simply guesses at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen-Kappa score: 0.9594886358546177\n"
     ]
    }
   ],
   "source": [
    "kappa = cohen_kappa_score(pred_lr, y_test)\n",
    "print(\"Cohen-Kappa score:\", kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one possible interpretation of Kappa:\n",
    "\n",
    "- Poor agreement = Less than 0.20 (including negative numbers)\n",
    "- Fair agreement = 0.20 to 0.40\n",
    "- Moderate agreement = 0.40 to 0.60\n",
    "- Good agreement = 0.60 to 0.80\n",
    "- Very good agreement = 0.80 to 1.00\n",
    "\n",
    "I recommend reading [this article](https://www.knime.com/blog/cohens-kappa-an-overview) and [this other article](https://analyticsindiamag.com/understanding-cohens-kappa-score-with-hands-on-implementation/) about Cohen’s Kappa to understand it better.\n",
    "\n",
    "There are, of course, other metrics that you can use on your ML model. I would suggest exploring them online.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epic 3: model improvement\n",
    "\n",
    "### Feature selection\n",
    "Now that you have a working wine type classification system, you can try to interpret one of these predictive models. One of the key aspects in model interpretation is to try to understand the importance of each feature from the dataset.\n",
    "\n",
    "[This article](https://machinelearningmastery.com/calculate-feature-importance-with-python/) shows how you can check feature importance for various algorithms.\n",
    "\n",
    "### Parameter tuning and model improvement with k-Fold cross-validation\n",
    "If during the evaluation you did not obtain good predictions, your model may be **overfitting or underfitting**. You must return to the training step and experiment with the hyperparameters in your model, before fitting and evaluating it again.\n",
    "\n",
    "Each algorithm has its parameters to adjust. Lots of patience and reading is required to improve your models. You will iterate the previous steps until you get satisfying results.\n",
    "\n",
    "You can use the k-Fold Validation to perform a validation of the training set for hyper-parameter tuning (check the Machine Learning Fundamentals chapter on Cross-Validation for more details). For this task, you can use the scikit-learn method cross_val_score.\n",
    "\n",
    "Be aware that cross_val_score and cross_validate are different things.\n",
    "\n",
    "### Deploy your model\n",
    "Deploying a machine learning model it's very important so that other people can use it, and it will not run only on your laptop but on the clouds. It's not something we will cover for now.\n",
    "\n",
    "The most important thing is to make a presentation with your most important findings and the results of your ML model.\n",
    "\n",
    "Communication and storytelling are very important soft skills.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epic 4: Try different algorithms and pick the best\n",
    "Repeat the process by choosing other algorithms, compare their results and pick the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epic 5: Repeat the process to predict the quality of the wines\n",
    "Repeat Sprint 3 and 4 in order to predict the quality label for the wines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
